#!/usr/bin/env python3
"""
merge_calendars.py
Download multiple .ics feeds listed in a JSON file, prefix each event’s
summary with a calendar-specific prefix, merge feeds into one .ics file, and
— when two events share the *exact* same start, end *and* all‑day status —
combine their summaries instead of writing duplicate events.

Key rule update (2025‑06‑17):
    • Keep the DTSTART/DTEND exactly as they appear in the first calendar that
      contributes an event.  We no longer convert to UTC when generating the
      duplicate‑detection key, because all feeds are in the same zone and we
      want to preserve that zone verbatim in the merged output.

Usage:
    python3 merge_calendars.py  [--json calendar_sources.json]  [--output combined.ics]

Dependencies:
    pip install requests icalendar pytz
"""

from __future__ import annotations

import argparse
import datetime as _dt
import json
import sys
from pathlib import Path
from typing import Dict, Tuple
from urllib.parse import urlparse

import requests
from icalendar import Calendar, Event, vText

JSON_DEFAULT = "calendar_sources.json"
OUTPUT_DEFAULT = "combined.ics"
HEADERS = {"User-Agent": "Calendar-Merger/1.2 (+https://example.com)"}

###############################################################################
# Helper functions
###############################################################################

def normalise_url(raw_url: str) -> str:
    """Strip any leading text before the first literal “http”."""
    http_pos = raw_url.find("http")
    if http_pos == -1:
        raise ValueError(f"Invalid URL string: {raw_url!r}")
    return raw_url[http_pos:]


def download_ics(url: str) -> Calendar:
    """Fetch an .ics feed and return it as an icalendar.Calendar object."""
    resp = requests.get(url, headers=HEADERS, timeout=30)
    resp.raise_for_status()
    return Calendar.from_ical(resp.content)


###############################################################################
# Duplicate‑detection utilities
###############################################################################

def _stringify_dt(dt_obj):
    """Return a comparable string form of DTSTART/DTEND without changing zones.

    * Datetimes keep their original tzinfo and are rendered with .isoformat().
    * Dates (all‑day events) are kept as ISO date strings.
    """
    return dt_obj.isoformat()


def event_key(event: Event) -> Tuple[str, str | None, bool]:
    """Return a hashable key identifying an event’s timing.

    key = (dtstart_iso, dtend_iso_or_None, is_all_day)
    """
    dtstart = event.decoded("DTSTART")
    dtend = event.decoded("DTEND") if "DTEND" in event else None

    is_all_day = not isinstance(dtstart, _dt.datetime)
    return (
        _stringify_dt(dtstart),
        _stringify_dt(dtend) if dtend else None,
        is_all_day,
    )

###############################################################################
# Main merge routine
###############################################################################

def main(json_path: Path, output_path: Path) -> None:
    # Read calendar sources list
    with json_path.open(encoding="utf-8") as fh:
        sources = json.load(fh)

    # Create a fresh calendar for the merge
    merged = Calendar()
    merged.add("prodid", "-//Merged via merge_calendars.py//EN")
    merged.add("version", "2.0")

    # Maps duplicate‑detection keys → Event already written to `merged`
    seen_events: Dict[Tuple[str, str | None, bool], Event] = {}

    for src in sources:
        if not src.get("Enabled", False):
            continue  # skip disabled feeds

        name   = src.get("Name", "Unnamed")
        prefix = src.get("Prefix", "")
        try:
            url = normalise_url(src["URL"])
        except (KeyError, ValueError) as e:
            print(f"Skipping {name}: {e}", file=sys.stderr)
            continue

        print(f"→ Downloading “{name}” from {urlparse(url).netloc} …")
        try:
            cal = download_ics(url)
        except Exception as e:
            print(f"   Failed ({e!s}); skipping.", file=sys.stderr)
            continue

        for component in cal.walk():
            if component.name != "VEVENT":
                # Copy VTIMEZONE etc. directly (avoid duping VCALENDAR)
                if component.name not in {"VCALENDAR"}:
                    merged.add_component(component)
                continue

            # Clone the event to avoid cross‑calendar side‑effects
            new_event = Event.from_ical(component.to_ical())

            # Prefix the summary
            summary = new_event.get("summary", vText(""))
            prefixed_summary = summary if str(summary).startswith(prefix) else f"{prefix}{summary}"
            new_event["summary"] = vText(prefixed_summary)

            # Duplicate detection & merging
            key = event_key(new_event)
            if key in seen_events:
                existing = seen_events[key]
                existing_summary = str(existing["summary"])
                if prefixed_summary not in existing_summary.split(", "):
                    existing["summary"] = vText(f"{existing_summary}, {prefixed_summary}")
                continue  # Don’t add a duplicate VEVENT component

            # First time we see this timing → add to merged output
            merged.add_component(new_event)
            seen_events[key] = new_event

    # Write out the combined file
    with output_path.open("wb") as fh:
        fh.write(merged.to_ical())

    print(f"\n✅ Done. Merged calendar written to: {output_path}")

###############################################################################
# CLI entry‑point
###############################################################################

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description=(
            "Merge multiple .ics feeds, deduplicating events with identical "
            "timing while preserving the original time‑zone of the first "
            "event encountered."
        )
    )
    parser.add_argument(
        "--json",
        default=JSON_DEFAULT,
        type=Path,
        help=f"Path to JSON definition file (default: {JSON_DEFAULT})",
    )
    parser.add_argument(
        "--output",
        default=OUTPUT_DEFAULT,
        type=Path,
        help=f"Output .ics file path (default: {OUTPUT_DEFAULT})",
    )
    args = parser.parse_args()

    main(args.json, args.output)
